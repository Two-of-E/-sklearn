### 人工智能低谷
- 硬件的计算能力
- 数据的大小
- 算法的发展
### 机器学习就是从数据中自动分析获得规律（模型），并利用规律对未知数据进行预测。
### 数据集的构成
- 机器学习的数据：csv文件
- MySQL：
  - 性能瓶颈，读取速度遭到限制
  - 格式不符合机器学习要求的格式
- pandas：读取数据
  - 读取速度快
- numpy：真正的多线程，释放了GIL锁
  - scipy:
    - sparse矩阵（节约内存，方便处理，用的不多）
    - one-hot编码
- 数据集的结构 特征值（给定的数据）+目标值(要确定的未知量)
- 数据的特征工程
  - 提高预测的效果
  - 数据集里对特征的抽取
    - 针对特征进行处理 
      - pandas
      - sklearn
      - 文本：转为数字化符号
      - dataFrame 
        - 缺失值，数据转换
        - 重复值，不需要去重
      - 文本特征抽取
        - count：
          - countvectorizer
          - 调用fit_transform方法输入数据并进行转换
          - 对于单个英文字母不统计
          - 对中文抽取要先进行分词
            - import jieba
          - 用途：
            - 文本分析，文本分类
            - 情感分析
        - tf-idf:评估词汇的重要性
          - tf：term frequency 词的频率
          - idf：逆文档频率
    - 数据的特征预处理
      - 缺失值处理
        - 插补：每列（按特征）的平均值、中位数进行填补
        - 函数：sklearn.preprocessing.Imputer
      - 标准化
        - 场景：样本足够多，适合现代嘈杂大数据场景
        - 异常数据很多时
        - feature(): 
      - 归一化
        - 特点：通过对原始数据进行交换把数据映射到（默认为[0,1]之间）
        - 公式
          - $X^/$ = $\frac{x-min}{max-min}$
          - $X^{//}$ = $X^/$ * (mx-mi) + mi
            - 指定区间默认mx为1，mi为0
          - $X^{//}$为最终值
        - 用途
          - 算法要求多个特征同等重要
            - 保证一个特征对最终结果不会造成更大的影响

    - 数据的降维
      - 降低特征的数量
        - 特征选择
          - API
          - 方差进行特征选择
        - 主成分分析
          - 分析简化数据集，
          - PCA：特征数量达到上百
          - 可以削减回归分析或者聚类分析中特征的数量
          - 原因
            - 噪声
            - 冗余
  
    - 数据类型
      - 离散型数据
        - 区间内不可分
      - 连续型数据 
        - 区间内可分
### 过拟合特征选择
- 过滤式：
  - 低方差特征
- 嵌入式
  - 正则化：
    - l2正则化：多次尝试缩小不同的高次项，进而选取优项
    - 超参数：alpha
  - 决策树 
  - 神经网络
### 算法分类
- 监督学习
  - 有特征值加目标值
  - 分类
    - 目标值离散
  - 回归
    - 目标值连续
- 非监督学习
  - 只有特征值
- 机器学习开发流程
  - 数据
    - 来源
      - 公司本身数据
      - 合作过来
      - 购买
  - 明确问题，建立模型
  - 数据基本处理： pd处理 
  - 特征工程
  - 找到合适的算法进行分析和预测
  - 定位模型：算法+模型
  - 模型的评估，判定效果 
  - 上线API
- sklearn优缺点
  - 优：封装好
  - 缺：参数少
- Tensorflow:封装高低都有
- sklearn数据集
  - 分为训练和测试
  - 估计器  estimator
- k-近邻算法 KNN
  - 优点：简单，无需估计参数，无需训练
  - k值的大小
    - 大：受k值数量波动
    - 小：容易受异常影响
  - 性能优化
    - 时间复杂度
  - 根据你的邻居判断自己的类型
  - 求解欧氏距离作为参考 
  - 需要做标准化
- 朴素贝叶斯
  - 主要应用于文本分类
  - 案例：
    - 垃圾邮件分类：对邮件内容分析，通过概率预估文件类型
  - 算法前提
    - 特征独立：即满足联合概率和条件概率
  - 拉普拉斯平滑
  - 优点：
    - 对缺失数据不敏感
    - 算法简单
    - 分类准确度高速度快
- 决策树
  - 优点：适用于各种数据，很好的分析能力，需要很少的数据准备，其他技术通常需要数据归一化
  - 缺点：发生过拟合
  - 改进：
    - 剪枝cart，减掉过拟合的情况，相当于一个去噪的滤波器
    - 集成学习方法
      - 多个分类器结合
      - 随机森林
        - 几乎没有缺点
        - 优点
          - 运用于大数据集
          - 极高的准确率
          - 几乎不需要降维
          - 评估特征的重要性
        - 多个决策树投票取优选项
        - 建立多个决策树的过程
          - N个样本，M个特征
          - 随机有放回的抽样
          - 随机在N个样本中选择一个样本，重复N次，样本可能重复
          - 在M中随机选出m（<<M）个特征
          - 
  - 可视化树的结构
  - 信息熵
    - 信息熵越大，不确定性越大
  - 决策树的划分
    - 信息增益（ID3）：一个特征条件可以减少的信息熵的大小
      - 增益较大的要放在靠近根节点处进行分类
    - 基尼系数：划分更加仔细，在sklearn里为默认项
    - 
- 线性回归
  - 自身属性
    - 目标值连续
    - 寻找可以预测的趋势
  - 属性和权重的组合
    - 矩阵：大多数算法计算基础
      - 二维
  - 回归性能评价：均方误差
  - 预测
    - 单变量
    - 多变量
  - 过拟合和欠拟合
    - 根据交叉验证训练集的结果
    - 欠拟合：训练集和测试集都表现不行
    - 过拟合：训练集好测试集不行
  - 迭代的算法
    - 知道误差，也不断的减少误差（改变权重，寻找最优化的W）
    - 策略：损失函数：最小二乘法
      - 正规方程
    - 学习率
- 逻辑回归：分类算法
    - 应用场景：二分类问题,不适合做多分类
    - softmax多分类图像识别（神经网络）
    - 参数：正则化力度
- 非监督学习
  - 聚类学习：只知道特征，没有目标值
  - k-mean
- 生成模型和判别模型
  - 先验概率：是指根据以往经验和分析得到的概率，如全概率公式，它往往作为"由因求果"问题中的"因"出现的概率。
    - 有先验概率可以称为生成模型
  - knn 决策树 随机森林 神经网络
  - 朴素贝叶斯 隐马尔科夫模型
### 分类模型的评估
- 对模型性能的评估
- 混淆矩阵
  - 二分类问题
  - 对角线正确
- 评估标准
  - 准确率
  - 精确率：预测结果为正例样本中真实为正例（查的准）
  - 召回率：真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力）
### 模型选择和调优
- 交叉验证：将拿到的训练数据分为训练和测试集，重复的求取平均值
  - 10折交叉验证
- 网格搜索调超参数
  - K-近邻
  - 超参数搜索 

